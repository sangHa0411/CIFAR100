{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o_P77SAV4y-A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q5aKLHGa4_R3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qXrJ0_Ym5A1Y"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-bqlDTwn5Cgx"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG6qDQv79ciC"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "dnVtzRyK9efi",
    "outputId": "96873e5e-46bd-477e-d6f1-f87d2bc3b1c9"
   },
   "outputs": [],
   "source": [
    "path =  '../Preprocessing/Data/'\n",
    "\n",
    "img_data = np.load(path + 'img_data.npy')\n",
    "img_label = np.load(path + 'img_age.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(img_data)\n",
    "rand_idx = np.random.choice(data_size , data_size , replace = False)\n",
    "\n",
    "train_size = int(0.8*data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = img_data[rand_idx]\n",
    "img_label = img_label[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = img_data[:train_size]\n",
    "label_train = img_label[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = img_data[train_size:]\n",
    "label_test = img_label[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape\n",
      "\n",
      "(2160, 7, 512, 384, 3)\n",
      "(2160, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Train Data Shape\\n')\n",
    "\n",
    "print(img_train.shape)\n",
    "print(label_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Shape\n",
      "\n",
      "(540, 7, 512, 384, 3)\n",
      "(540, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Test Data Shape\\n')\n",
    "\n",
    "print(img_test.shape)\n",
    "print(label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 512\n",
    "width = 384\n",
    "channel = 3\n",
    "a_class = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = img_train.reshape(-1 , height , width , channel)\n",
    "label_train = label_train.reshape(-1,)\n",
    "\n",
    "img_test = img_test.reshape(-1 , height , width , channel)\n",
    "label_test = label_test.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "re_size = 224\n",
    "batch_size = 32\n",
    "\n",
    "mid_h = int(height/2)\n",
    "mid_w = int(width/2)\n",
    "mid_s = int(img_size/2)\n",
    "\n",
    "# crop image of center : image shape (18900 , 256 , 256 , 3)\n",
    "img_train = img_train[: , \n",
    "                      mid_h - mid_s : mid_h + mid_s , \n",
    "                      mid_w - mid_s : mid_w + mid_s ,\n",
    "                      :]\n",
    "\n",
    "img_test = img_test[: , \n",
    "                    mid_h - mid_s : mid_h + mid_s , \n",
    "                    mid_w - mid_s : mid_w + mid_s ,\n",
    "                    :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uENPRUJM5FVe"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_RW65s2d5GUX"
   },
   "outputs": [],
   "source": [
    "class ProjectDataset(Dataset) :\n",
    "\n",
    "    def __init__(self , data , label , class_size) :\n",
    "\n",
    "        super(Dataset , self).__init__()\n",
    "\n",
    "        self.data = np.transpose(data , (0,3,1,2)) # change channel first (for pytorch)\n",
    "        self.label = np.eye(class_size)[label.astype('int32')] # one hot vector\n",
    "\n",
    "    def __len__(self) :\n",
    "\n",
    "        data_len = self.data.shape[0]\n",
    "\n",
    "        return data_len\n",
    "\n",
    "    def __getitem__(self , idx) :\n",
    "\n",
    "        data_idx = self.data[idx]\n",
    "        label_idx = self.label[idx]\n",
    "        \n",
    "        # return dict type\n",
    "        sample_idx = {'image' : data_idx , 'label' : label_idx}\n",
    "        \n",
    "        return sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XgXYDm489qGF"
   },
   "outputs": [],
   "source": [
    "# train dataset\n",
    "image_train_dset = ProjectDataset(img_train, label_train , a_class)\n",
    "\n",
    "# train data loader\n",
    "image_train_loader = DataLoader(image_train_dset, \n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True ,\n",
    "                                num_workers=4 ,\n",
    "                                drop_last=True)\n",
    "\n",
    "# test dataset\n",
    "image_test_dset = ProjectDataset(img_test, label_test , a_class)\n",
    "\n",
    "# test data loader\n",
    "image_test_loader = DataLoader(image_test_dset,  \n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False , \n",
    "                               num_workers=4 ,\n",
    "                               drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CutMix :\n",
    "    \n",
    "    def __init__(self, img_height , img_width):\n",
    "        \n",
    "        self.h = img_height\n",
    "        self.w = img_width \n",
    "        \n",
    "        # combination rate sampled from the uniform distribution\n",
    "        self.gen = torch.distributions.beta.Beta(1,1)\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        # org image\n",
    "        a_image, a_label = sample['image'], sample['label']\n",
    "        \n",
    "        batch_size = len(a_image)\n",
    "        rand = torch.randperm(batch_size)\n",
    "        \n",
    "        # image which we will take patch\n",
    "        b_image = a_image[rand]\n",
    "        b_label = a_label[rand]\n",
    "        \n",
    "        # y , x point of patch\n",
    "        y = torch.randint(self.h , (1,))[0]\n",
    "        x = torch.randint(self.w, (1,))[0]\n",
    "\n",
    "        # combination ratio\n",
    "        r = self.gen.sample()\n",
    "        \n",
    "        # height and width of patch\n",
    "        h = (self.h * torch.sqrt(1-r)).int()\n",
    "        w = (self.w * torch.sqrt(1-r)).int()\n",
    "\n",
    "        # org image(a) + image patch(b)\n",
    "        c_image = a_image.clone()\n",
    "        c_image[: , : , y:y+h , x:x+w] = b_image[: , : ,y:y+h , x:x+w]\n",
    "        \n",
    "        # combine label\n",
    "        c_label = a_label * r + b_label * (1-r)\n",
    "        \n",
    "        return {'image' : c_image , 'label' : c_label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoCwmMV65OZ6"
   },
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Q924OvyO5PUy"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "random.seed(20210905)\n",
    "torch.cuda.manual_seed_all(20210905)\n",
    "\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVwMHFh05RGO"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9sxuGpWCORBe"
   },
   "outputs": [],
   "source": [
    "# resnet 101 layer\n",
    "layer_dim = [3,4,23,3]\n",
    "ch_dim = [64,128,256,512]\n",
    "\n",
    "start_ch = 64\n",
    "start_k = 7\n",
    "inter_k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartBlock(nn.Module) :\n",
    "    \n",
    "    def __init__(self , start_ch , start_k) :\n",
    "        \n",
    "        super(StartBlock , self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(nn.Conv2d(3,start_ch,start_k,stride=2,padding=int(start_k/2)),\n",
    "                                 nn.BatchNorm2d(start_ch),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(3 , stride=2 , padding=1))\n",
    "        \n",
    "    def forward(self , in_tensor) :\n",
    "        \n",
    "        o_tensor = self.net(in_tensor)\n",
    "        \n",
    "        return o_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module) :\n",
    "    \n",
    "    def __init__(self, layer , in_ch , conv_ch , kernal) :\n",
    "        \n",
    "        super(ConvBlock , self).__init__()\n",
    "        \n",
    "        self.layer = layer\n",
    "        self.img_size = img_size\n",
    "        self.in_ch = in_ch\n",
    "        self.conv_ch = conv_ch\n",
    "        self.out_ch = conv_ch * 4\n",
    "        self.kernal = kernal\n",
    "\n",
    "        self.conv_net = nn.ModuleList()\n",
    "        \n",
    "        if self.in_ch != self.out_ch :\n",
    "            self.flag_11 = True \n",
    "            self.conv_11 = nn.Conv2d(in_ch,self.out_ch,1)\n",
    "        else :\n",
    "            self.flag_11 = False\n",
    "        \n",
    "        for i in range(layer) :\n",
    "            \n",
    "            ch_ptr = in_ch if i == 0 else self.out_ch\n",
    "            \n",
    "            conv_seq = nn.Sequential(nn.Conv2d(ch_ptr,conv_ch,1), \n",
    "                                     nn.BatchNorm2d(conv_ch),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(conv_ch,conv_ch,self.kernal,padding=int(kernal/2)),\n",
    "                                     nn.BatchNorm2d(conv_ch),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(conv_ch,self.out_ch,1),\n",
    "                                     nn.BatchNorm2d(self.out_ch),\n",
    "                                     nn.ReLU())\n",
    "            self.conv_net.append(conv_seq)\n",
    "        \n",
    "    def forward(self, in_tensor) :\n",
    "        \n",
    "        tensor_ptr = in_tensor\n",
    "        p_tensor = self.conv_11(in_tensor) if self.flag_11 else in_tensor\n",
    "        \n",
    "        for i in range(self.layer) :\n",
    "            \n",
    "            h_tensor = self.conv_net[i](tensor_ptr)    \n",
    "            o_tensor = F.relu(h_tensor + p_tensor)\n",
    "            \n",
    "            tensor_ptr = o_tensor\n",
    "            p_tenosr = o_tensor\n",
    "            \n",
    "        return tensor_ptr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9DQX2EIYG0R6"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module) :\n",
    "\n",
    "    def __init__(self, img_size , layers , channels , kernal , \n",
    "                 in_ch , in_kernal , class_size) :\n",
    "\n",
    "        super(ResNet , self).__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.layers = layers\n",
    "        self.channels = channels\n",
    "        self.kernal = kernal\n",
    "        self.in_ch = in_ch\n",
    "        self.in_k = in_kernal\n",
    "        self.class_size = class_size\n",
    "        \n",
    "        self.start = StartBlock(in_ch , in_kernal)\n",
    "        self.resnet = nn.ModuleList()\n",
    "        self.tran = nn.ModuleList()\n",
    "        self.norm = nn.ModuleList()\n",
    "\n",
    "        size_ptr = img_size / 4\n",
    "        ch_ptr = in_ch\n",
    "        \n",
    "        for i in range(len(layers)) :\n",
    "\n",
    "            self.resnet.append(ConvBlock(layers[i] , ch_ptr , channels[i] , kernal))\n",
    "            ch_ptr = channels[i] * 4\n",
    "\n",
    "            if i < len(layers) - 1 :\n",
    "                self.tran.append(nn.Conv2d(ch_ptr,channels[i+1]*4,1,stride=2))\n",
    "                self.norm.append(nn.BatchNorm2d(channels[i+1]*4))\n",
    "                \n",
    "                ch_ptr = channels[i+1] * 4\n",
    "                size_ptr /= 2\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(int(size_ptr)) # final average pooling layer\n",
    "        self.o_layer = nn.Linear(channels[-1]*4 , class_size)\n",
    "\n",
    "        self.init_param()\n",
    "\n",
    "    def init_param(self) :\n",
    "\n",
    "        nn.init.kaiming_normal_(self.o_layer.weight)\n",
    "        nn.init.zeros_(self.o_layer.bias) \n",
    "        \n",
    "        for m in self.modules() :\n",
    "            if isinstance(m , nn.Conv2d) :\n",
    "\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self , in_tensor) :\n",
    "\n",
    "        batch_size = in_tensor.shape[0]\n",
    "        tensor_ptr = self.start(in_tensor)\n",
    "        \n",
    "        for i in range(len(self.resnet)) :\n",
    "    \n",
    "            # conv2 block\n",
    "            tensor_ptr = self.resnet[i](tensor_ptr)\n",
    "\n",
    "            if i < len(self.tran) :\n",
    "                # transition\n",
    "                tensor_ptr = self.tran[i](tensor_ptr)\n",
    "\n",
    "                # batch normalization and Relu activation\n",
    "                tensor_ptr = self.norm[i](tensor_ptr)\n",
    "                tensor_ptr = F.relu(tensor_ptr)\n",
    "\n",
    "        avg_tensor = self.avg_pool(tensor_ptr)\n",
    "        avg_tensor = torch.reshape(avg_tensor , (batch_size , self.channels[-1]*4))\n",
    "\n",
    "        o_tensor = self.o_layer(avg_tensor)\n",
    "    \n",
    "        return o_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "EKNEUK0wMrv2"
   },
   "outputs": [],
   "source": [
    "epoch_size = 100\n",
    "min_loss = 1e+7\n",
    "init_lr = 3e-4\n",
    "early_count = 0\n",
    "log_count = 0\n",
    "\n",
    "cutmix =  CutMix(img_size , img_size)\n",
    "\n",
    "scalor = transforms.Compose([transforms.Resize((re_size,re_size)),\n",
    "                             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))])\n",
    "\n",
    "resnet = ResNet(re_size , layer_dim , ch_dim , inter_k , start_ch , start_k , a_class).to(device)\n",
    "\n",
    "optimizer = optim.Adam(resnet.parameters() , lr = init_lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 3 , gamma = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_fn(y_output , y_label) :\n",
    "\n",
    "    # get max argument of output \n",
    "    _ , y_out_arg = torch.max(y_output , dim = -1)\n",
    "    _ , y_label_arg = torch.max(y_label , dim = -1)\n",
    "\n",
    "    # check if output max argument if same as output label\n",
    "    y_acc = (y_out_arg == y_label_arg).float()    \n",
    "    y_acc = torch.mean(y_acc)\n",
    "\n",
    "    return y_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_output , y_label) :\n",
    "    \n",
    "    y_prob = F.softmax(y_output , dim = -1)\n",
    "    y_log = -torch.log(y_prob + 1e-12)\n",
    "    \n",
    "    y_loss = torch.multiply(y_log , y_label)\n",
    "    y_loss = torch.sum(y_loss , dim = -1)\n",
    "    \n",
    "    y_loss = torch.mean(y_loss)\n",
    "    \n",
    "    return y_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/resnet/cutmix/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "_64VKVW-M0G4"
   },
   "outputs": [],
   "source": [
    "def progressLearning(value, endvalue, loss , acc , bar_length=50):\n",
    "      \n",
    "    percent = float(value + 1) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}/{2} \\t Loss : {3:.3f} , Acc : {4:.3f}\".format(arrow + spaces, value+1 , endvalue , loss , acc))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "1VRtShSeOx5-"
   },
   "outputs": [],
   "source": [
    "def evaluate(model , scalor  , test_loader , device) :\n",
    "\n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "\n",
    "    with torch.no_grad() :\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for img_data in test_loader :\n",
    "     \n",
    "            img_in , img_label = img_data['image'] , img_data['label']\n",
    "            img_in = img_in.float().to(device) / 255\n",
    "            img_in = scalor(img_in) # scalor\n",
    "            \n",
    "            img_label = img_label.float().to(device)\n",
    "\n",
    "            img_output = model(img_in)\n",
    "\n",
    "            loss_idx = loss_fn(img_output , img_label)\n",
    "            acc_idx = acc_fn(img_output , img_label)\n",
    "\n",
    "            loss += loss_idx\n",
    "            acc += acc_idx\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    loss /= len(test_loader)\n",
    "    acc /= len(test_loader)\n",
    "\n",
    "    return loss , acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dm-ISTtcOzc7",
    "outputId": "c3fddcad-ecc3-4279-e3de-14d34a651dc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 \t Learning Rate : 3.000000e-04\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 1.018 , Acc : 0.500\n",
      "Validation Loss : 0.6560 \t Validation Acc : 0.7018\n",
      "\n",
      "Epoch : 1 \t Learning Rate : 3.000000e-04\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.752 , Acc : 0.594\n",
      "Validation Loss : 0.6463 \t Validation Acc : 0.7873\n",
      "\n",
      "Epoch : 2 \t Learning Rate : 3.000000e-04\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.627 , Acc : 0.781\n",
      "Validation Loss : 0.6535 \t Validation Acc : 0.7823\n",
      "\n",
      "Epoch : 3 \t Learning Rate : 2.400000e-04\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.733 , Acc : 0.875"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_size) :\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    print('Epoch : %d \\t Learning Rate : %e' %(epoch , optimizer.param_groups[0]['lr'])) \n",
    "    \n",
    "    for img_data in image_train_loader : \n",
    "\n",
    "        img_data = cutmix(img_data)\n",
    "     \n",
    "        img_in , img_label = img_data['image'] , img_data['label']\n",
    "        img_in = img_in.float().to(device) / 255\n",
    "        img_in = scalor(img_in) # scalor\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        img_label = img_label.float().to(device)\n",
    "        \n",
    "        img_output = resnet(img_in) \n",
    "        \n",
    "        loss = loss_fn(img_output , img_label)  \n",
    "        acc = acc_fn(img_output , img_label) \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx + 1) % 10 == 0 :\n",
    "            \n",
    "            writer.add_scalar('train/loss' , loss.item() , log_count)\n",
    "            writer.add_scalar('train/acc' , acc.item() , log_count)\n",
    "            log_count += 1\n",
    "        \n",
    "        progressLearning(idx , len(image_train_loader) , loss, acc) \n",
    "\n",
    "        idx += 1 \n",
    "\n",
    "    test_loss, test_acc = evaluate(resnet, scalor , image_test_loader , device) \n",
    "    \n",
    "    writer.add_scalar('test/loss' , test_loss.item() , epoch)\n",
    "    writer.add_scalar('test/acc' , test_acc.item() , epoch)\n",
    "    \n",
    "    if test_loss < min_loss :\n",
    "        \n",
    "        min_loss = test_loss\n",
    "        torch.save({'epoch' : (epoch) ,  \n",
    "                    'model_state_dict' : resnet.state_dict() , \n",
    "                    'loss' : test_loss.item() , \n",
    "                    'acc' : test_acc.item()} , \n",
    "                    f'./Model/checkpoint_resnet_cutmix.pt')        \n",
    "        early_count = 0 \n",
    "        \n",
    "    else :\n",
    "        \n",
    "        early_count += 1\n",
    "        if early_count >= 5 :      \n",
    "            print('\\nTraining Stopped')\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print('\\nValidation Loss : %.4f \\t Validation Acc : %.4f\\n' %(test_loss , test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=./runs/resnet/  --port=6006 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e91086c794a4681a4a543bc84d12ca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b3b085e1ed148b58d465c4a54ed2c26",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2de9605e768425ea55629556e757fc1",
      "value": 169001437
     }
    },
    "0f5bc96e97fa46b6a0d2a761adebad28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ecb4b1683d94c56ba97a9878453af92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b3b085e1ed148b58d465c4a54ed2c26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54e8a11e0d3546fc99dbabcd1a268bf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_737313f8358a4d2f87991dbbd46978b8",
       "IPY_MODEL_0e91086c794a4681a4a543bc84d12ca1",
       "IPY_MODEL_c7b3715b1a944c6f891ccfc973f3a83c"
      ],
      "layout": "IPY_MODEL_2ecb4b1683d94c56ba97a9878453af92"
     }
    },
    "737313f8358a4d2f87991dbbd46978b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8392c76123647f09d92826ffbba869e",
      "placeholder": "​",
      "style": "IPY_MODEL_931f458e4472482b8405af3f1fd4fa8d",
      "value": ""
     }
    },
    "931f458e4472482b8405af3f1fd4fa8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d4cf3d529e747439aa7d49cd50ba17e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8392c76123647f09d92826ffbba869e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7b3715b1a944c6f891ccfc973f3a83c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f5bc96e97fa46b6a0d2a761adebad28",
      "placeholder": "​",
      "style": "IPY_MODEL_9d4cf3d529e747439aa7d49cd50ba17e",
      "value": " 169001984/? [00:04&lt;00:00, 47709790.44it/s]"
     }
    },
    "d2de9605e768425ea55629556e757fc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
