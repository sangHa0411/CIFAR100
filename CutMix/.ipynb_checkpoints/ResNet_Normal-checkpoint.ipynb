{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o_P77SAV4y-A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q5aKLHGa4_R3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qXrJ0_Ym5A1Y"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-bqlDTwn5Cgx"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG6qDQv79ciC"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "dnVtzRyK9efi",
    "outputId": "96873e5e-46bd-477e-d6f1-f87d2bc3b1c9"
   },
   "outputs": [],
   "source": [
    "path =  '../Preprocessing/Data/'\n",
    "\n",
    "img_data = np.load(path + 'img_data.npy')\n",
    "img_label = np.load(path + 'img_age.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(img_data)\n",
    "rand_idx = np.random.choice(data_size , data_size , replace = False)\n",
    "\n",
    "train_size = int(0.8*data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = img_data[rand_idx]\n",
    "img_label = img_label[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = img_data[:train_size]\n",
    "label_train = img_label[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = img_data[train_size:]\n",
    "label_test = img_label[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape\n",
      "\n",
      "(2160, 7, 512, 384, 3)\n",
      "(2160, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Train Data Shape\\n')\n",
    "\n",
    "print(img_train.shape)\n",
    "print(label_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Shape\n",
      "\n",
      "(540, 7, 512, 384, 3)\n",
      "(540, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Test Data Shape\\n')\n",
    "\n",
    "print(img_test.shape)\n",
    "print(label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 512\n",
    "width = 384\n",
    "channel = 3\n",
    "a_class = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = img_train.reshape(-1 , height , width , channel)\n",
    "label_train = label_train.reshape(-1,)\n",
    "\n",
    "img_test = img_test.reshape(-1 , height , width , channel)\n",
    "label_test = label_test.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 288\n",
    "re_size = 224\n",
    "batch_size = 32\n",
    "\n",
    "mid_h = int(height/2)\n",
    "mid_w = int(width/2)\n",
    "mid_s = int(img_size/2)\n",
    "\n",
    "# crop image of center : image shape (18900 , 256 , 256 , 3)\n",
    "img_train = img_train[: , \n",
    "                      mid_h - mid_s : mid_h + mid_s , \n",
    "                      mid_w - mid_s : mid_w + mid_s ,\n",
    "                      :]\n",
    "\n",
    "img_test = img_test[: , \n",
    "                    mid_h - mid_s : mid_h + mid_s , \n",
    "                    mid_w - mid_s : mid_w + mid_s ,\n",
    "                    :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uENPRUJM5FVe"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_RW65s2d5GUX"
   },
   "outputs": [],
   "source": [
    "class ProjectDataset(Dataset) :\n",
    "\n",
    "    def __init__(self , data , label , class_size) :\n",
    "\n",
    "        super(Dataset , self).__init__()\n",
    "\n",
    "        self.data = np.transpose(data , (0,3,1,2)) # change channel first (for pytorch)\n",
    "        self.label = np.eye(class_size)[label.astype('int32')] # one hot vector\n",
    "\n",
    "    def __len__(self) :\n",
    "\n",
    "        data_len = self.data.shape[0]\n",
    "\n",
    "        return data_len\n",
    "\n",
    "    def __getitem__(self , idx) :\n",
    "\n",
    "        data_idx = self.data[idx]\n",
    "        label_idx = self.label[idx]\n",
    "        \n",
    "        # return dict type\n",
    "        sample_idx = {'image' : data_idx , 'label' : label_idx}\n",
    "        \n",
    "        return sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XgXYDm489qGF"
   },
   "outputs": [],
   "source": [
    "# train dataset\n",
    "image_train_dset = ProjectDataset(img_train, label_train , a_class)\n",
    "\n",
    "# train data loader\n",
    "image_train_loader = DataLoader(image_train_dset, \n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True ,\n",
    "                                num_workers=4 ,\n",
    "                                drop_last=True)\n",
    "\n",
    "# test dataset\n",
    "image_test_dset = ProjectDataset(img_test, label_test , a_class)\n",
    "\n",
    "# test data loader\n",
    "image_test_loader = DataLoader(image_test_dset,  \n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False , \n",
    "                               num_workers=4 ,\n",
    "                               drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoCwmMV65OZ6"
   },
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Q924OvyO5PUy"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "random.seed(20210905)\n",
    "torch.cuda.manual_seed_all(20210905)\n",
    "\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVwMHFh05RGO"
   },
   "source": [
    "## Model Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9sxuGpWCORBe"
   },
   "outputs": [],
   "source": [
    "# 논문 기반 \n",
    "\n",
    "# Resnet Convolution layer 수\n",
    "layer_dim = [2,3,5,2]\n",
    "# Resnet Convolution Filter 갯수\n",
    "ch_dim = [64,128,256,512]\n",
    "start_kernal = 7 # 시작 Kernal 수\n",
    "kernal_size = 3 # Convolution Kernal 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXWDgR47OXM_"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "STDAgxwzAdhy"
   },
   "outputs": [],
   "source": [
    "class Conv2ResidualSame(nn.Module) :\n",
    "\n",
    "    # input channel and output channel are smae\n",
    "    def __init__(self, ch_size , k_size) :\n",
    "\n",
    "        super(Conv2ResidualSame , self).__init__()\n",
    "\n",
    "        pad_size = int(k_size/2)\n",
    "        # 출력 모양와 입력 모양이 같게 하기 위해 Padding Size를 Kernal 사이즈의 절반으로 설정\n",
    "        self.c_res = nn.Sequential(nn.Conv2d(ch_size,ch_size,k_size,padding=pad_size,stride=1),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(ch_size,ch_size,k_size,padding=pad_size,stride=1))\n",
    "        \n",
    "        self.init_param()\n",
    "\n",
    "    # 파라미터 초기화\n",
    "    def init_param(self) :\n",
    "\n",
    "        for m in self.modules() :\n",
    "            if isinstance(m , nn.Conv2d) :\n",
    "\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "    def forward(self , in_tensor) :\n",
    "        \n",
    "        out_tensor = self.c_res(in_tensor)\n",
    "        # 입력과 출력결과를 더함\n",
    "        res_tensor = torch.add(out_tensor , in_tensor)\n",
    "\n",
    "        return res_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UvPgxyGBD-u_"
   },
   "outputs": [],
   "source": [
    "class Conv2ResidualDiff(nn.Module) :\n",
    "\n",
    "    # input channel and output channel are different\n",
    "    def __init__(self, in_ch , out_ch , k_size) :\n",
    "\n",
    "        super(Conv2ResidualDiff , self).__init__()\n",
    "\n",
    "        pad_size = int(k_size/2)\n",
    "\n",
    "        # stride size is 2 and channel size is doubled \n",
    "        self.c_res = nn.Sequential(nn.Conv2d(in_ch,out_ch,k_size,padding=pad_size,stride=2),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(out_ch,out_ch,k_size,padding=pad_size,stride=1))\n",
    "        \n",
    "\n",
    "        # 1 X 1 Convolution to match input dimension and out dimension\n",
    "        self.conv11 = nn.Conv2d(in_ch,out_ch,1,stride=2) \n",
    "\n",
    "        self.init_param()\n",
    "\n",
    "    def init_param(self) :\n",
    "\n",
    "        for m in self.modules() :\n",
    "            if isinstance(m , nn.Conv2d) :\n",
    "\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self , in_tensor) :\n",
    "\n",
    "        # 출력 모양의 채널 갯수가 커지고 입출력 길이가 절반으로 감소\n",
    "        out_tensor = self.c_res(in_tensor)\n",
    "        # 입력과 출력의 모양을 맞추기 위해서 1X1 Convolution을 이용해서 채널 수와 입출력 모양을 일치시킴\n",
    "        project_tensor = self.conv11(in_tensor)\n",
    "\n",
    "        # 입력과 출력을 더함\n",
    "        res_tensor = torch.add(out_tensor , project_tensor)\n",
    "\n",
    "        return res_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sCy5HJQG5Sa1"
   },
   "outputs": [],
   "source": [
    "class Conv2Block(nn.Module) :\n",
    "\n",
    "    def __init__(self, layer_size , in_ch , ch_size , k_size) :\n",
    "\n",
    "        super(Conv2Block , self).__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "\n",
    "        self.c_block = nn.ModuleList()\n",
    "        # Convolutional Residual block (output channel is doubled and feature map size is halvec) \n",
    "        self.c_block.append(Conv2ResidualDiff(in_ch,ch_size,k_size))\n",
    "\n",
    "        # Convolutional Residual block (input channel and output channel is same and also with feature map size)\n",
    "        for i in range(layer_size) :\n",
    "            self.c_block.append(Conv2ResidualSame(ch_size,k_size))\n",
    "\n",
    "    def forward(self , in_tensor) :\n",
    "\n",
    "        tensor_ptr = in_tensor\n",
    "\n",
    "        for i in range(self.layer_size) :\n",
    "\n",
    "            tensor_ptr = self.c_block[i](tensor_ptr)\n",
    "\n",
    "        return tensor_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9DQX2EIYG0R6"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module) :\n",
    "\n",
    "    def __init__(self, layer_list , image_size , ch_list , \n",
    "                 in_kernal , kernal_size , class_size) :\n",
    "\n",
    "        super(ResNet , self).__init__()\n",
    "\n",
    "        self.ch_list = ch_list\n",
    "        self.resnet = nn.ModuleList()\n",
    "\n",
    "        size_ptr = image_size\n",
    "        # first convolutional layer\n",
    "        self.resnet.append(nn.Conv2d(3,ch_dim[0],in_kernal,stride=2,padding=int(in_kernal/2))) \n",
    "        size_ptr /= 2\n",
    "\n",
    "        ch_ptr = ch_dim[0]\n",
    "        for i in range(len(layer_list)) :\n",
    "\n",
    "            # convolution residual block\n",
    "            convblock = Conv2Block(layer_list[i],ch_ptr,ch_dim[i],kernal_size)\n",
    "            self.resnet.append(convblock) \n",
    "            self.resnet.append(nn.BatchNorm2d(ch_dim[i])) # batch normalization\n",
    "\n",
    "            size_ptr /= 2\n",
    "            ch_ptr = ch_dim[i]\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(int(size_ptr)) # final average pooling layer\n",
    "        self.o_layer = nn.Linear(ch_list[-1] , class_size)\n",
    "        \n",
    "        self.layer_size = len(self.resnet)\n",
    "\n",
    "        self.init_param()\n",
    "\n",
    "    def init_param(self) :\n",
    "\n",
    "        nn.init.kaiming_normal_(self.o_layer.weight)\n",
    "        nn.init.zeros_(self.o_layer.bias) \n",
    "\n",
    "    def forward(self , in_tensor) :\n",
    "\n",
    "        batch_size = in_tensor.shape[0]\n",
    "        tensor_ptr = in_tensor\n",
    "\n",
    "        for i in range(self.layer_size) :\n",
    "\n",
    "            tensor_ptr = self.resnet[i](tensor_ptr)\n",
    "\n",
    "        avg_tensor = self.avg_pool(tensor_ptr)\n",
    "        avg_tensor = torch.reshape(avg_tensor , (batch_size , self.ch_list[-1]))\n",
    "\n",
    "        o_tensor = self.o_layer(avg_tensor)\n",
    "    \n",
    "        return o_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "EKNEUK0wMrv2"
   },
   "outputs": [],
   "source": [
    "epoch_size = 200\n",
    "min_loss = 1e+7\n",
    "init_lr = 1e-4\n",
    "early_count = 0\n",
    "log_count = 0\n",
    "\n",
    "scalor = transforms.Compose([transforms.Resize((re_size,re_size)),\n",
    "                             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))])\n",
    "\n",
    "resnet = ResNet(layer_dim , re_size , ch_dim , start_kernal , kernal_size , a_class).to(device)\n",
    "\n",
    "optimizer = optim.Adam(resnet.parameters() , lr = init_lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 3 , gamma = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_fn(y_output , y_label) :\n",
    "\n",
    "    # get max argument of output \n",
    "    _ , y_out_arg = torch.max(y_output , dim = -1)\n",
    "    _ , y_label_arg = torch.max(y_label , dim = -1)\n",
    "\n",
    "    # check if output max argument if same as output label\n",
    "    y_acc = (y_out_arg == y_label_arg).float()    \n",
    "    y_acc = torch.mean(y_acc)\n",
    "\n",
    "    return y_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_output , y_label) :\n",
    "    \n",
    "    y_prob = F.softmax(y_output , dim = -1)\n",
    "    y_log = -torch.log(y_prob + 1e-12)\n",
    "    \n",
    "    y_loss = torch.multiply(y_log , y_label)\n",
    "    y_loss = torch.sum(y_loss , dim = -1)\n",
    "    \n",
    "    y_loss = torch.mean(y_loss)\n",
    "    \n",
    "    return y_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/resnet/normal/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_64VKVW-M0G4"
   },
   "outputs": [],
   "source": [
    "def progressLearning(value, endvalue, loss , acc , bar_length=50):\n",
    "      \n",
    "    percent = float(value + 1) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}/{2} \\t Loss : {3:.3f} , Acc : {4:.3f}\".format(arrow + spaces, value+1 , endvalue , loss , acc))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1VRtShSeOx5-"
   },
   "outputs": [],
   "source": [
    "def evaluate(model , scalor  , test_loader , device) :\n",
    "\n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "\n",
    "    with torch.no_grad() :\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for img_data in test_loader :\n",
    "            \n",
    "            img_in , img_label = img_data['image'] , img_data['label']\n",
    "\n",
    "            img_in = img_in.float().to(device) / 255\n",
    "            img_in = scalor(img_in) # scalor\n",
    "            \n",
    "            img_label = img_label.float().to(device)\n",
    "\n",
    "            img_output = model(img_in)\n",
    "\n",
    "            loss_idx = loss_fn(img_output , img_label)\n",
    "            acc_idx = acc_fn(img_output , img_label)\n",
    "\n",
    "            loss += loss_idx\n",
    "            acc += acc_idx\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    loss /= len(test_loader)\n",
    "    acc /= len(test_loader)\n",
    "\n",
    "    return loss , acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dm-ISTtcOzc7",
    "outputId": "c3fddcad-ecc3-4279-e3de-14d34a651dc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 \t Learning Rate : 1.000000e-04\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.495 , Acc : 0.812\n",
      "Validation Loss : 1.0957 \t Validation Acc : 0.5776\n",
      "\n",
      "Epoch : 1 \t Learning Rate : 1.000000e-04\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.388 , Acc : 0.875\n",
      "Validation Loss : 0.4144 \t Validation Acc : 0.8382\n",
      "\n",
      "Epoch : 2 \t Learning Rate : 1.000000e-04\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.163 , Acc : 0.938\n",
      "Validation Loss : 0.4084 \t Validation Acc : 0.8700\n",
      "\n",
      "Epoch : 3 \t Learning Rate : 8.000000e-05\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.190 , Acc : 0.906\n",
      "Validation Loss : 0.5133 \t Validation Acc : 0.8385\n",
      "\n",
      "Epoch : 4 \t Learning Rate : 8.000000e-05\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.083 , Acc : 0.938\n",
      "Validation Loss : 0.5086 \t Validation Acc : 0.8493\n",
      "\n",
      "Epoch : 5 \t Learning Rate : 8.000000e-05\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.037 , Acc : 0.969\n",
      "Validation Loss : 0.5145 \t Validation Acc : 0.8734\n",
      "\n",
      "Epoch : 6 \t Learning Rate : 6.400000e-05\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.007 , Acc : 1.000\n",
      "Validation Loss : 0.5348 \t Validation Acc : 0.8464\n",
      "\n",
      "Epoch : 7 \t Learning Rate : 6.400000e-05\n",
      "Percent: [------------------------------------------------->] 472/472 \t Loss : 0.002 , Acc : 1.000\n",
      "Training Stopped\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_size) :\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    print('Epoch : %d \\t Learning Rate : %e' %(epoch , optimizer.param_groups[0]['lr'])) \n",
    "    \n",
    "    for img_data in image_train_loader : \n",
    "\n",
    "        img_in , img_label = img_data['image'] , img_data['label']\n",
    "        \n",
    "        img_in = img_in.float().to(device) / 255\n",
    "        img_in = scalor(img_in) # scalor\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        img_label = img_label.float().to(device)\n",
    "        \n",
    "        img_output = resnet(img_in) \n",
    "        \n",
    "        loss = loss_fn(img_output , img_label)  \n",
    "        acc = acc_fn(img_output , img_label) \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx + 1) % 10 == 0 :\n",
    "            \n",
    "            writer.add_scalar('train/loss' , loss.item() , log_count)\n",
    "            writer.add_scalar('train/acc' , acc.item() , log_count)\n",
    "            log_count += 1\n",
    "        \n",
    "        progressLearning(idx , len(image_train_loader) , loss, acc) \n",
    "\n",
    "        idx += 1 \n",
    "\n",
    "    test_loss, test_acc = evaluate(resnet, scalor , image_test_loader , device) \n",
    "    \n",
    "    writer.add_scalar('test/loss' , test_loss.item() , epoch)\n",
    "    writer.add_scalar('test/acc' , test_acc.item() , epoch)\n",
    "    \n",
    "    if test_loss < min_loss :\n",
    "        \n",
    "        min_loss = test_loss\n",
    "        torch.save({'epoch' : (epoch) ,  \n",
    "                    'model_state_dict' : resnet.state_dict() , \n",
    "                    'loss' : test_loss.item() , \n",
    "                    'acc' : test_acc.item()} , \n",
    "                    f'./Model/checkpoint_resnet_normal.pt')        \n",
    "        early_count = 0 \n",
    "        \n",
    "    else :\n",
    "                \n",
    "        early_count += 1\n",
    "        if early_count >= 5 :      \n",
    "            print('\\nTraining Stopped')\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print('\\nValidation Loss : %.4f \\t Validation Acc : %.4f\\n' %(test_loss , test_acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-05 23:01:56.862055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-05 23:01:56.867681: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-09-05 23:01:56.868442: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-09-05 23:01:56.868477: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "TensorBoard 2.4.1 at http://abf6efe47668:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./runs/resnet/ --port=6006 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e91086c794a4681a4a543bc84d12ca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b3b085e1ed148b58d465c4a54ed2c26",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2de9605e768425ea55629556e757fc1",
      "value": 169001437
     }
    },
    "0f5bc96e97fa46b6a0d2a761adebad28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ecb4b1683d94c56ba97a9878453af92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b3b085e1ed148b58d465c4a54ed2c26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54e8a11e0d3546fc99dbabcd1a268bf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_737313f8358a4d2f87991dbbd46978b8",
       "IPY_MODEL_0e91086c794a4681a4a543bc84d12ca1",
       "IPY_MODEL_c7b3715b1a944c6f891ccfc973f3a83c"
      ],
      "layout": "IPY_MODEL_2ecb4b1683d94c56ba97a9878453af92"
     }
    },
    "737313f8358a4d2f87991dbbd46978b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8392c76123647f09d92826ffbba869e",
      "placeholder": "​",
      "style": "IPY_MODEL_931f458e4472482b8405af3f1fd4fa8d",
      "value": ""
     }
    },
    "931f458e4472482b8405af3f1fd4fa8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d4cf3d529e747439aa7d49cd50ba17e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8392c76123647f09d92826ffbba869e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7b3715b1a944c6f891ccfc973f3a83c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f5bc96e97fa46b6a0d2a761adebad28",
      "placeholder": "​",
      "style": "IPY_MODEL_9d4cf3d529e747439aa7d49cd50ba17e",
      "value": " 169001984/? [00:04&lt;00:00, 47709790.44it/s]"
     }
    },
    "d2de9605e768425ea55629556e757fc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
